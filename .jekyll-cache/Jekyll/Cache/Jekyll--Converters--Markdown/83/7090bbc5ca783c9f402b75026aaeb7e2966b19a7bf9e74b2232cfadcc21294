I"Â'<h2 id="accelerators">Accelerators</h2>

<p>Accelerators represent hardware or software GPU devices.
They store information about different devices and allow memory allocation and kernel loading on a particular device.
A launch of a kernel on an accelerator is performed asynchronously by default.
Synchronization with the accelerator or the associated stream is required in order to to wait for completion and to fetch results.</p>

<p>Note that instances of classes that depend on an accelerator reference have to be disposed before disposing of the associated accelerator object.
However, this does not apply to automatically managed kernels, which are cached inside the accelerator object.</p>

<div class="language-c# highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="err">...</span>
<span class="err">{</span>
    <span class="nc">static</span> <span class="k">void</span> <span class="nf">Main</span><span class="p">(</span><span class="kt">string</span><span class="p">[]</span> <span class="n">args</span><span class="p">)</span>
    <span class="p">{</span>
        <span class="c1">// Initialize a new ILGPU context</span>
        <span class="k">using</span> <span class="nn">var</span> <span class="n">context</span> <span class="p">=</span> <span class="k">new</span> <span class="nf">Context</span><span class="p">();</span>

        <span class="k">using</span> <span class="p">(</span><span class="kt">var</span> <span class="n">cpuAccelerator</span> <span class="p">=</span> <span class="k">new</span> <span class="nf">CPUAccelerator</span><span class="p">(</span><span class="n">context</span><span class="p">);</span>
        <span class="c1">// Perform operations on the CPU</span>

        <span class="k">using</span> <span class="nn">var</span> <span class="n">cudaAccelerator</span> <span class="p">=</span> <span class="k">new</span> <span class="nf">CudaAccelerator</span><span class="p">(</span><span class="n">context</span><span class="p">);</span>
        <span class="c1">// Perform operations on the default Cuda device</span>

        <span class="c1">// Iterate over all available accelerators</span>
        <span class="k">foreach</span> <span class="p">(</span><span class="kt">var</span> <span class="n">acceleratorId</span> <span class="k">in</span> <span class="n">Accelerator</span><span class="p">.</span><span class="n">Accelerators</span><span class="p">)</span>
        <span class="p">{</span>
            <span class="k">using</span> <span class="p">(</span><span class="kt">var</span> <span class="n">accl</span> <span class="p">=</span> <span class="n">Accelerator</span><span class="p">.</span><span class="nf">Create</span><span class="p">(</span><span class="n">context</span><span class="p">,</span> <span class="n">acceleratorId</span><span class="p">))</span>
            <span class="p">{</span>
                <span class="c1">// Perform operations</span>
            <span class="p">}</span>
        <span class="p">}</span>
    <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div>

<p>You can print detailed accelerator information to the stdout stream by invoking the <code class="language-plaintext highlighter-rouge">PrintInformation</code> method. This yields output similar to the following. Sample output of an RTX 3090 using <code class="language-plaintext highlighter-rouge">accelerator.PrintInformation()</code>:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Device: GeForce RTX 3090 [ILGPU InstanceId: 20]
  Cuda device id:                          0
  Cuda driver version:                     11.2
  Cuda architecture:                       SM_86
  Instruction set:                         7.1
  Clock rate:                              1860 MHz
  Memory clock rate:                       9751 MHz
  Memory bus width:                        384-bit
  Number of multiprocessors:               82
  Max number of threads/multiprocessor:    1536
  Max number of threads/group:             1024
  Max number of total threads:             125952
  Max dimension of a group size:           (1024, 1024, 64)
  Max dimension of a grid size:            (2147483647, 65535, 65535)
  Total amount of global memory:           25769803776 bytes, 24576 MB
  Total amount of constant memory:         65536 bytes, 64 KB
  Total amount of shared memory per group: 49152 bytes, 48 KB
  Total amount of shared memory per mp:    102400 bytes, 100 KB
  L2 cache size:                           6291456 bytes, 6144 KB
  Max memory pitch:                        2147483647 bytes
  Total number of registers per mp:        65536
  Total number of registers per group:     65536
  Concurrent copy and kernel execution:    True, with 2 copy engines
  Driver mode:                             WDDM
  Has ECC support:                         False
  Supports managed memory:                 True
  Supports compute preemption:             True
  PCI domain id / bus id / device id:      0 / 11 / 0
  NVML PCI bus id:                         0000:0B:00.0
</code></pre></div></div>

<h2 id="cuda-and-opencl-accelerators">Cuda and OpenCL Accelerators</h2>

<p>The current Cuda (PTX) backend supports different driver and feature levels.
The Cuda backend does not require a Cuda SDK to be installed/configured.</p>

<p>An automatic driver detection module selects an appropriate PTX ISA version for your graphics driver.
However, if you encounter the error message <code class="language-plaintext highlighter-rouge">A PTX jit compilation failed</code> try updating the graphics driver first before diving deeper into this issue.</p>

<p>Use <code class="language-plaintext highlighter-rouge">CudaAccelerator.CudaAccelerators</code> to query all Cuda-compatible GPUs in your system. Use <code class="language-plaintext highlighter-rouge">CLAccelerator.CLAccelerators</code> to query all OpenCL-compatible GPUs in your system.</p>

<p>It is <strong>highly recommended</strong> to use the <code class="language-plaintext highlighter-rouge">CudaAccelerator</code> class for NVIDIA GPUs and the <code class="language-plaintext highlighter-rouge">CLAccelerator</code> class for Intel and AMD GPUs.</p>

<div class="language-c# highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">using</span> <span class="nn">ILGPU.Runtime.Cuda</span><span class="p">;</span>
<span class="k">using</span> <span class="nn">ILGPU.Runtime.OpenCL</span><span class="p">;</span>
<span class="k">class</span> <span class="err">...</span>
<span class="err">{</span>
    <span class="nc">static</span> <span class="k">void</span> <span class="nf">Main</span><span class="p">(</span><span class="kt">string</span><span class="p">[]</span> <span class="n">args</span><span class="p">)</span>
    <span class="p">{</span>
        <span class="p">...</span>
        <span class="kt">var</span> <span class="n">allSupportedAccelerators</span> <span class="p">=</span> <span class="n">Accelerator</span><span class="p">.</span><span class="n">Accelerators</span><span class="p">;</span>
        <span class="kt">var</span> <span class="n">cudaAccelerators</span> <span class="p">=</span> <span class="n">CudaAccelerator</span><span class="p">.</span><span class="n">CudaAccelerators</span><span class="p">;</span>
        <span class="kt">var</span> <span class="n">supportedCLAccelerators</span> <span class="p">=</span> <span class="n">CLAccelerator</span><span class="p">.</span><span class="n">CLAccelerators</span><span class="p">;</span>
        <span class="kt">var</span> <span class="n">allCLAccelerators</span> <span class="p">=</span> <span class="n">CLAccelerator</span><span class="p">.</span><span class="n">AllCLAccelerators</span><span class="p">;</span>
    <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div>

<h2 id="streams">Streams</h2>

<p><code class="language-plaintext highlighter-rouge">AcceleratorStreams</code> represent async operation queues, which operations can be submitted to.
Custom accelerator streams have to be synchronized manually.
Using streams increases the parallellism of applications.
Every accelerator encapsulates a default accelerator stream that is used for all operations by default.</p>

<div class="language-c# highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="err">...</span>
<span class="err">{</span>
    <span class="nc">static</span> <span class="k">void</span> <span class="nf">Main</span><span class="p">(</span><span class="kt">string</span><span class="p">[]</span> <span class="n">args</span><span class="p">)</span>
    <span class="p">{</span>
        <span class="p">...</span>

        <span class="kt">var</span> <span class="n">defaultStream</span> <span class="p">=</span> <span class="n">accelerator</span><span class="p">.</span><span class="n">DefaultStream</span><span class="p">;</span>
        <span class="k">using</span> <span class="p">(</span><span class="kt">var</span> <span class="n">secondStream</span> <span class="p">=</span> <span class="n">accelerator</span><span class="p">.</span><span class="nf">CreateStream</span><span class="p">())</span>
        <span class="p">{</span>

            <span class="c1">// Perform actions using default stream...</span>

            <span class="c1">// Perform actions on second stream...</span>

            <span class="c1">// Wait for results from the first stream.</span>
            <span class="n">defaultStream</span><span class="p">.</span><span class="nf">Synchronize</span><span class="p">();</span>

            <span class="c1">// Use results async compared to operations on the second stream...</span>

            <span class="c1">// Wait for results from the second stream</span>
            <span class="n">secondStream</span><span class="p">.</span><span class="nf">Synchronize</span><span class="p">();</span>

            <span class="p">...</span>
        <span class="p">}</span>
    <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div>
:ET